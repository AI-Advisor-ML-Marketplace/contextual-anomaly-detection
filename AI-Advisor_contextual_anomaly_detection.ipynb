{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train, tune, and deploy a custom ML model using <font color='red'>AIA(AI-Advisor) Univariate Contextual Anomaly Detection(U-CAD) </font> Algorithm from AWS Marketplace \n",
    "\n",
    "⚠ **Change the link for the `AIA-PAD-ALGO` the current link is our gitlab link**\n",
    "\n",
    "<font color='red'> This product is able to detect contextual anomalies in time-series data that exhibit seasonal or cyclic patterns using unsupervised machine learning algorithms. It also offers auto hyperparameter optimization to provide users with greater convenience in finding proper hyperparameters for the algorithms and creating optimized anomaly detection models for their data. Additionally, the algorithm employs techniques to improve the accuracy of basic contextual anomaly detection algorithms. </font>\n",
    "\n",
    "This sample notebook shows you how to train a U-CAD using <font color='red'> For Seller to update: [AIA-PAD-ALGO](http://mod.lge.com/hub/ai_contents_marketplace/aia-ml-marketplace)</font> from AWS Marketplace.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "## Pre-requisites\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. Some hands-on experience using [Amazon SageMaker](https://aws.amazon.com/sagemaker/).\n",
    "1. To use this algorithm successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to <font color='red'> For Seller to update: [AIA-PAD-ALGO](http://mod.lge.com/hub/ai_contents_marketplace/aia-ml-marketplace)</font>. \n",
    "\n",
    "## Contents\n",
    "1. [Subscribe to the algorithm](#1.-Subscribe-to-the-algorithm)  \n",
    "2. [Prepare dataset](#2.-Prepare-dataset)  \n",
    "    2.1 [Dataset format expected by the algorithm](#2.1-Dataset-format-expected-by-the-algorithm)  \n",
    "\t2.2 [Configure and visualize train and test dataset](#2.2-Configure-and-visualize-train-and-test-dataset)  \n",
    "\t2.3 [Upload datasets to Amazon S3](#2.3-Upload-datasets-to-Amazon-S3)  \n",
    "3. [Train a machine learning model](#3:-Train-a-machine-learning-model)  \n",
    "\t3.1 [Set up environment](#3.1-Set-up-environment)  \n",
    "\t3.2 [Train a model](#3.2-Train-a-model)    \n",
    "4. [Deploy model and verify results](#4.-Deploy-model-and-verify-results)  \n",
    "    4.1 [Deploy trained model](#4.1-Deploy-trained-model)  \n",
    "    4.2 [Visualize output](#4.2-Create-input-payload)   \n",
    "5. [Perform inference](#5.-Perform-inference)  \n",
    "    5.1 [Perform batch inference](#5.1-Perform-batch-inference)  \n",
    "    5.2 [Visualize output](#5.2-Visualize-output)  \n",
    "    5.3 [Perform real-time inference](#5.3-Perform-real-time-inference)    \n",
    "    5.4 [Visualize output](#5.4-Visualize-output)  \n",
    "    5.5 [Delete the endpoint](#5.3-Delete-the-endpoint)  \n",
    "6. [Clean-up](#6.-Clean-up)  \n",
    "\t6.1 [Delete the model](#A.-Delete-the-model)  \n",
    "    6.2 [Unsubscribe to the listing (optional)](#B.-Unsubscribe-to-the-listing-(optional))\n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the algorithm:\n",
    "1. Open the algorithm listing page <font color='red'> For Seller to update: [AIA-PAD-ALGO](http://mod.lge.com/hub/ai_contents_marketplace/aia-ml-marketplace)</font>\n",
    "1. On the AWS Marketplace listing,  click on **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you agree with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn**. This is the algorithm ARN that you need to specify while training a custom ML model. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![product_arn_image](images/product_arn_image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from getpass import getpass \n",
    "\n",
    "# SHAPE\n",
    "# algo_arn = \"<Customer to specify algorithm ARN corresponding to their AWS region follow the instruction above>\"\n",
    "\n",
    "########################################CHANGE####################################################\n",
    "# SAMPLE\n",
    "algo_arn='arn:aws:sagemaker:us-east-2:438613450817:algorithm/cad-image-v1-1-0'\n",
    "##################################################################################################\n",
    "\n",
    "# get your seesion information\n",
    "#####################################################\n",
    "aws_region = \"us-east-2\"  ##\n",
    "aws_access_key = getpass(prompt=\"Access key: \")\n",
    "aws_secret_key = getpass(prompt=\"Secret key: \")\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import uuid\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import ModelPackage\n",
    "from urllib.parse import urlparse\n",
    "import boto3\n",
    "from IPython.display import Image\n",
    "from PIL import Image as ImageEdit\n",
    "import urllib.request\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset format expected by the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution follows these **2 steps**:  `Training` and `Testing` the algorithm.\n",
    "\n",
    "**Train**\n",
    "- The algorithm trains on user provided dataset.\n",
    "- Dataset must be in `csv` shape, under `./data/train/` folder, with 'utf-8' encoding.\n",
    "\n",
    "**Test**\n",
    "- After the Machine Learning model is trained, it can be used to make prediction using test dataset.\n",
    "- The algorithm also tests on user provided dataset.\n",
    "- Dataset must be in `csv` shape, under `./data/test/` folder, with 'utf-8' encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data format**  \n",
    "To input data into the U-CAD, it is necessary for the data to have three columns.\n",
    "- time column: time column must be in the format of '%Y-%m-%dT%H:%M:%S'. In case your data does not have a time column, you can create a fake time column and add it in your dataframe. \n",
    "- data column: data column should contain the data that you want to analyze.\n",
    "- cycle column: cycle column distinguishes the pattern in your data. If your data is a collection of patterns with the same length, you can use the code below(def make_cycle_column). However, if the patterns in your data have different lengths, it will be necessary to create a cycle column and manually add it to your dataframe. It is important to note that the data within one pattern must be recorded in the same value.  \n",
    "\n",
    "*tip*\n",
    "- There's no need to worry if the patterns within your data have varying lengths. The U-CAD system can handle it! To ensure that the patterns are analyzed correctly, the system will perform an interpolation process to align the patterns and make them uniform in length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Configure and visualize train and test dataset\n",
    "The `train` and `test` dataset should look like this as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd # import padas to show how data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SHAPE\n",
    "# training_dataset = \"data/train/<FileName.ext>\"\n",
    "\n",
    "########################################CHANGE####################################################\n",
    "# SAMPLE\n",
    "training_dataset = \"data/train/real_ucr_1sddb40_train.csv\" # 300point cycle\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_fake</th>\n",
       "      <th>data</th>\n",
       "      <th>cycle</th>\n",
       "      <th>project</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-18T08:19:37</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-18T08:19:47</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-18T08:19:57</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-18T08:20:07</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-18T08:20:17</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             time_fake   data  cycle  project\n",
       "0  2023-03-18T08:19:37  170.0    0.0  project\n",
       "1  2023-03-18T08:19:47  171.0    0.0  project\n",
       "2  2023-03-18T08:19:57  175.0    0.0  project\n",
       "3  2023-03-18T08:20:07  171.0    0.0  project\n",
       "4  2023-03-18T08:20:17  175.0    0.0  project"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show sample of training dataset\n",
    "train_df = pd.read_csv(training_dataset)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SHAPE\n",
    "# test_dataset = \"data/test/<FileName.ext>\"\n",
    "\n",
    "########################################CHANGE####################################################\n",
    "# SAMPLE\n",
    "test_dataset = \"data/test/real_ucr_1sddb40_inf.csv\"\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_fake</th>\n",
       "      <th>data</th>\n",
       "      <th>cycle</th>\n",
       "      <th>project</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-18T08:19:37</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-18T08:19:47</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-18T08:19:57</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-18T08:20:07</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-18T08:20:17</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             time_fake   data  cycle  project\n",
       "0  2023-03-18T08:19:37  128.0    0.0  project\n",
       "1  2023-03-18T08:19:47  135.0    0.0  project\n",
       "2  2023-03-18T08:19:57  138.0    0.0  project\n",
       "3  2023-03-18T08:20:07  146.0    0.0  project\n",
       "4  2023-03-18T08:20:17  146.0    0.0  project"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show sample of test dataset\n",
    "test_df = pd.read_csv(test_dataset)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data you're working with doesn't have a cycle column and you know the pattern length, proceed accordingly.  \n",
    "In case the patterns in your data have varying lengths, you'll need to add the cycle column manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_cycle_column(df,pattern_length):\n",
    "    return np.concatenate([np.ones(pattern_length)*i for i in range(int(len(df)//pattern_length)+1)])[:len(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['cycle'] = make_cycle_column(train_df,300)\n",
    "test_df['cycle'] = make_cycle_column(test_df,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.to_csv(training_dataset,index=False)\n",
    "test_df.to_csv(test_dataset,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Upload datasets to Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Do not change bucket parameter value. Do not hardcode your S3 bucket name.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-2-438613450817'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "\n",
    "boto_session = boto3.Session(region_name=aws_region, aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key)\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session) # get session info\n",
    "\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input uploaded to : s3://sagemaker-us-east-2-438613450817/TRAINING_INPUT_DIR/real_ucr_1sddb40_train.csv\n"
     ]
    }
   ],
   "source": [
    "# upload training data to s3 bucket\n",
    "training_data = sagemaker_session.upload_data(training_dataset, bucket=bucket, key_prefix=\"TRAINING_INPUT_DIR\")\n",
    "print(\"Training input uploaded to : \" + training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # upload test data to s3 bucket\n",
    "# test_data = sagemaker_session.upload_data(test_dataset, bucket=bucket, key_prefix=\"INFERENCE_INPUT_DIR\")\n",
    "# print(\"Inference input uploaded to : \" + test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that dataset is available in an accessible Amazon S3 bucket, we are ready to train a machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account ID: 438613450817\n"
     ]
    }
   ],
   "source": [
    "## If you are running on a local server, enter the role name specified in IAM role.\n",
    "\n",
    "sts = boto3.client('sts', region_name=aws_region, aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key)\n",
    "caller_identity = sts.get_caller_identity()\n",
    "account_id = caller_identity['Account']\n",
    "role_name = input(\"Role name: \")\n",
    "role = f'arn:aws:iam::{account_id}:role/{role_name}'\n",
    "\n",
    "\n",
    "\n",
    "### If you are running in sagemaker jupyter notebook then uncomment the below. (The above is commented out.) \n",
    "\n",
    "#from sagemaker import get_execution_role\n",
    "#role = get_execution_role(sagemaker_session=sagemaker_session)\n",
    "\n",
    "print (f\"Result: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>For Seller to update: update algorithm sepcific unique prefix in following cell. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SHAPE\n",
    "# output_location = \"s3://{}/<For seller to Update:Update a unique prefix>/{}\".format(bucket, \"output\")\n",
    "\n",
    "########################################CHANGE####################################################\n",
    "# SAMPLE\n",
    "output_location = \"s3://{}/ai-advisor-cad/{}\".format(bucket, \"output\")\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find more information about dataset format in **Hyperparameters** section of <font color='red'> For Seller to update:[Title_of_your_product](Provide link to your marketplace listing of your product).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**  \n",
    "you must specify the column names and choose which CAD functions to enable or disable in hyperparameters  \n",
    "Here is a description of the arguments that work in CAD\n",
    "1. **common_x_columns**\n",
    "    - Please include the name of the data column to be analyzed in the input data.\n",
    "2. **common_time_columns**\n",
    "    - Please include the name of the time column in your input data.\n",
    "3. **common_index_columns**\n",
    "    - In CAD, you can create models for each group using the 'groupkey' function with the 'common_index_columns' parameter.\n",
    "    - Groupkey column is the column of groups. The groupkey column specifies the groups, and the data within the same group should have the same value.\n",
    "    - You can set the 'groupkey' up to 3 columns by using the 'common_index_columns' parameter with the following argument format   \n",
    "    *Argument format*: 'column1,column2,column3'.\n",
    "4. **common_pattern_column** \n",
    "    - Please include the name of the cycle column in your input data\n",
    "5. **common_max_pattern_length** \n",
    "    - If the data has varying pattern lengths and cycle column lengths, and you know the cycle length, you can use the optional function in CAD to interpolate the data into a common maximum pattern length  \n",
    "    *Argument format*: ex. '200'\n",
    "6. **common_missing_x_adjust**\n",
    "    - This function handles missing values in the data column.\n",
    "    *Argument format*: 'none','dropna'(drop the missing values)\n",
    "7. **inference_adaptive_threshold**\n",
    "    - In CAD, we use the 'w' argument as a threshold, which ranges from 1 to 9 (float). By setting the value of 'w', the anomaly threshold is adjusted adaptively based on the input data.\n",
    "    - Checking the 'w_table.csv' file after training allows you to choose the appropriate value for 'w'. If you leave it blank, CAD will automatically set the value for 'w' in the inference that was found during hyperparameter optimization (HPO) in training.  \n",
    "    *Argument format*: ex. '2.5' \n",
    "8. **infernece_update_model**\n",
    "    - If you want to update the model with new inference data, just set the value to 'true' and the model will be updated accordingly. On the other hand, if you don't want to modify the model after inference, just set the value to 'false'.  \n",
    "    *Argument format*: 'true','false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SHAPE\n",
    "# hyperparameters = {}\n",
    "\n",
    "########################################CHANGE####################################################\n",
    "# Define hyperparameters\n",
    "hyperparameters = {'common_x_columns': 'data',\n",
    "                   'common_time_column': 'time_fake',\n",
    "                   'common_index_columns': '',\n",
    "                   'common_pattern_column': 'cycle',\n",
    "                   'common_max_pattern_length':'',\n",
    "                   'common_missing_x_adjust':'none',\n",
    "                   'inference_adaptive_threshold':'',\n",
    "                   'inference_update_model':'true'}\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>For Seller to update: Update appropriate values in estimator definition and ensure that fit call works as expected.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information on creating an `Estimator` object, see [documentation](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-438613450817/TRAINING_INPUT_DIR/real_ucr_1sddb40_train.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-30 01:26:20 Starting - Starting the training job...\n",
      "2023-03-30 01:26:44 Starting - Preparing the instances for trainingProfilerReport-1680139580: InProgress\n",
      "...\n",
      "2023-03-30 01:27:24 Downloading - Downloading input data...\n",
      "2023-03-30 01:27:45 Training - Downloading the training image.........\n",
      "2023-03-30 01:29:30 Training - Training image download completed. Training in progress..\u001b[34mjson_path:  /opt/ml/input/config/hyperparameters.json\u001b[0m\n",
      "\u001b[34myaml_path:  /opt/program/framework/configure/cad.train.workflow.yaml\u001b[0m\n",
      "\u001b[34mcurrent mode :  train\u001b[0m\n",
      "\u001b[34m##############Train Hyperparameters overload complete##############\u001b[0m\n",
      "\u001b[34mjson_path:  /opt/ml/input/config/hyperparameters.json\u001b[0m\n",
      "\u001b[34myaml_path:  /opt/program/framework/configure/cad.inference.workflow.yaml\u001b[0m\n",
      "\u001b[34mcurrent mode :  inference\u001b[0m\n",
      "\u001b[34m##############Inference Hyperparameters overload complete##############\u001b[0m\n",
      "\u001b[34mTrain, Inference hyperparams overriden!\u001b[0m\n",
      "\u001b[34maip yaml file is replaced with aip_train yaml file!\u001b[0m\n",
      "\u001b[34mStart Train Pipeline!\u001b[0m\n",
      "\u001b[34m2023-03-30 01:29:34.771408: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-03-30 01:29:34.771440: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m#033[95m***************************  INIT WORKFLOW  *********************************************#033[0m\u001b[0m\n",
      "\u001b[34m#033[95m[SIMULATOR SETTING : VERSION(0.2.1)]#033[0m\u001b[0m\n",
      "\u001b[34m#033[95m - NAMESPACE_NAME     : ai-advisor#033[0m\u001b[0m\n",
      "\u001b[34m#033[95m - PROJECT_NAME       : vs_telematcis#033[0m\u001b[0m\n",
      "\u001b[34m#033[95m - WORFLOW_CRONTAB    : 0#033[0m\u001b[0m\n",
      "\u001b[34m#033[95m - WORFLOW_TYPE       : cad#033[0m\u001b[0m\n",
      "\u001b[34m#033[95m - WORFLOW_YAML       : cad.train.workflow.yaml#033[0m\u001b[0m\n",
      "\u001b[34m#033[95m[WORKFLOW SETTING]#033[0m\u001b[0m\n",
      "\u001b[34m#033[95m - WORKFLOW_NAME      : cad-train#033[0m\u001b[0m\n",
      "\u001b[34m#033[95m - WORKFLOW STEP      : ['INIT', 'INPUT', 'VALIDATE', 'PREPROCESS', 'EXTRACT', 'TRAIN', 'OUTPUT']#033[0m\u001b[0m\n",
      "\u001b[34m#033[95m*****************************************************************************************#033[0m\u001b[0m\n",
      "\u001b[34m#033[95m*****************************************************************************************#033[0m\u001b[0m\n",
      "\u001b[34m#033[95m***************************  START WORKFLOW  ********************************************#033[0m\u001b[0m\n",
      "\u001b[34m#033[95m*****************************************************************************************#033[0m\u001b[0m\n",
      "\u001b[34m#033[92m\u001b[0m\n",
      "\u001b[34m################################# STEP INIT #####################################033[0m\n",
      " ARGUMENTS  : ['/opt/program/framework/scripts/data_init/tabular/asset_init.py', 'global_model_type=cad', 'global_pipeline_type=train', 'x_columns=data', 'y_column=', 'time_column=time_fake', 'ignore_columns=', 'index_columns=project', 'input_type=batch', 'purpose=cad_train']\u001b[0m\n",
      "\u001b[34m#033[94m=========================== METADATA STORE ============================#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mSTORE TYPE    : WORKFLOW#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mDATABASE TYPE : SQLITE#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mDATABASE FILE : /opt/program/framework/.storage/cad-train/cad-train.db#033[0m\u001b[0m\n",
      "\u001b[34m#033[94m=======================================================================#033[0m\u001b[0m\n",
      "\u001b[34m#033[94m========================== ASSET INFORMATION ==========================#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mTIME(KST)    : 2023-03-30 10:29:35#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKSPACE    : ai-advisor#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mPROJECT      : vs_telematcis#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKFLOW     : cad-train#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKFLOW KEY : vs_telematcis-cad-train-20230330012935#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mASSET NAME   : init#033[0m\u001b[0m\n",
      "\u001b[34m#033[94masset ver.   : v.1.1.0#033[0m\u001b[0m\n",
      "\u001b[34m#033[94maiplib ver.  : 1.1.0#033[0m\u001b[0m\n",
      "\u001b[34m#033[94maiptfx ver.  : 0.6.4#033[0m\u001b[0m\n",
      "\u001b[34m#033[94m=======================================================================#033[0m\u001b[0m\n",
      "\u001b[34m************************************************************\u001b[0m\n",
      "\u001b[34m#011init -> run\u001b[0m\n",
      "\u001b[34m************************************************************\u001b[0m\n",
      "\u001b[34m{'aiplib': '1.1.0', 'asset': 'v.1.1.0', 'aiptfx': '0.6.4'}\u001b[0m\n",
      "\u001b[34m[METADATA][LOG][INFO] : y_column은 비어있습니다.\u001b[0m\n",
      "\u001b[34m#033[92m\u001b[0m\n",
      "\u001b[34m################################# STEP INPUT #####################################033[0m\n",
      " ARGUMENTS  : ['/opt/program/framework/scripts/data_input/tabular/asset_input.py', 'input_type=batch', 'input_path=/opt/ml/input/data/training/', 'gather_delay=0', 'date=2023-03-30T01:29:35', 'data_length=5000', 'data_delay=0']\u001b[0m\n",
      "\u001b[34m#033[94m=========================== METADATA STORE ============================#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mSTORE TYPE    : WORKFLOW#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mDATABASE TYPE : SQLITE#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mDATABASE FILE : /opt/program/framework/.storage/cad-train/cad-train.db#033[0m\u001b[0m\n",
      "\u001b[34m#033[94m=======================================================================#033[0m\u001b[0m\n",
      "\u001b[34m[METADATA][LOG][INFO] : version check success!!\u001b[0m\n",
      "\u001b[34m#033[94m========================== ASSET INFORMATION ==========================#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mTIME(KST)    : 2023-03-30 10:29:36#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKSPACE    : ai-advisor#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mPROJECT      : vs_telematcis#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKFLOW     : cad-train#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKFLOW KEY : vs_telematcis-cad-train-20230330012935#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mASSET NAME   : input#033[0m\u001b[0m\n",
      "\u001b[34m#033[94masset ver.   : v.1.1.0#033[0m\u001b[0m\n",
      "\u001b[34m#033[94maiplib ver.  : 1.1.0#033[0m\u001b[0m\n",
      "\u001b[34m#033[94maiptfx ver.  : 0.6.4#033[0m\u001b[0m\n",
      "\u001b[34m#033[94m=======================================================================#033[0m\u001b[0m\n",
      "\u001b[34m************************************************************\u001b[0m\n",
      "\u001b[34m#011input -> run\u001b[0m\n",
      "\u001b[34m************************************************************\u001b[0m\n",
      "\u001b[34m{'aiplib': '1.1.0', 'aiptfx': '0.6.4', 'asset': 'v.1.1.0'}\u001b[0m\n",
      "\u001b[34m[METADATA][LOG][INFO] : 입력된 날짜는 2023-03-30T01:29:36 입니다\u001b[0m\n",
      "\u001b[34m[METADATA][LOG][INFO] : 사용되는 날짜 포맷은 %Y-%m-%dT%H:%M:%S 입니다\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34m>>>>> Load path : /opt/ml/input/data/training/\u001b[0m\n",
      "\u001b[34m>>>>> Load data : [2023-03-30T01:20:00 <= data < 2023-03-30T01:20:00]\u001b[0m\n",
      "\u001b[34m>>>>> Success loading data\u001b[0m\n",
      "\u001b[34m[METADATA][LOG][INFO] : input_path 에 s3 에 포함되어 있지 않다면, nas/efs 로 부터 데이터를 불러 옵니다. (해당 path:/opt/ml/input/data/training/)\u001b[0m\n",
      "\u001b[34m[METADATA][LOG][INFO] : Batch 데이터에 해당하는 파일를 load 하였습니다. (파일명: real_ucr_1sddb40_train.csv)\u001b[0m\n",
      "\u001b[34m2023-03-30 01:29:36.147361: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-03-30 01:29:36.147396: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2023-03-30 01:29:36.147416: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-176-133.us-east-2.compute.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2023-03-30 01:29:36.147715: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34mSaved : /opt/program/framework/workflows/vs_telematcis-cad-train-20230330012935/interface/input.tfrecord\u001b[0m\n",
      "\u001b[34m#033[92m\u001b[0m\n",
      "\u001b[34m################################# STEP VALIDATE #####################################033[0m\n",
      " ARGUMENTS  : ['/opt/program/framework/scripts/data_readiness/adxai/asset_readiness.py', 'outlier_by_sigma=10', 'outlier_by_percent=1', 'missing_ratio_allowed=1']\u001b[0m\n",
      "\u001b[34m#033[94m=========================== METADATA STORE ============================#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mSTORE TYPE    : WORKFLOW#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mDATABASE TYPE : SQLITE#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mDATABASE FILE : /opt/program/framework/.storage/cad-train/cad-train.db#033[0m\u001b[0m\n",
      "\u001b[34m#033[94m=======================================================================#033[0m\u001b[0m\n",
      "\u001b[34m[METADATA][LOG][INFO] : version check success!!\u001b[0m\n",
      "\u001b[34m#033[94m========================== ASSET INFORMATION ==========================#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mTIME(KST)    : 2023-03-30 10:29:36#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKSPACE    : ai-advisor#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mPROJECT      : vs_telematcis#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKFLOW     : cad-train#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKFLOW KEY : vs_telematcis-cad-train-20230330012935#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mASSET NAME   : validate#033[0m\u001b[0m\n",
      "\u001b[34m#033[94masset ver.   : None#033[0m\u001b[0m\n",
      "\u001b[34m#033[94maiplib ver.  : 1.1.0#033[0m\u001b[0m\n",
      "\u001b[34m#033[94maiptfx ver.  : 0.6.4#033[0m\u001b[0m\n",
      "\u001b[34m#033[94m=======================================================================#033[0m\u001b[0m\n",
      "\u001b[34m2023-03-30 01:29:36.947108: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\u001b[0m\n",
      "\u001b[34m2023-03-30 01:29:36.964247: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2999995000 Hz\u001b[0m\n",
      "\u001b[34mLoaded : /opt/program/framework/workflows/vs_telematcis-cad-train-20230330012935/interface/input.tfrecord\u001b[0m\n",
      "\u001b[34m************************************************************\u001b[0m\n",
      "\u001b[34m#011validate -> run\u001b[0m\n",
      "\u001b[34m************************************************************\u001b[0m\n",
      "\u001b[34m{'asset': 'None', 'aiplib': '1.1.0', 'aiptfx': '0.6.4'}\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mSaved : /opt/program/framework/workflows/vs_telematcis-cad-train-20230330012935/interface/validate.tfrecord\u001b[0m\n",
      "\u001b[34m#033[92m\u001b[0m\n",
      "\u001b[34m################################# STEP PREPROCESS #####################################033[0m\n",
      " ARGUMENTS  : ['/opt/program/framework/scripts/data_preprocess/tabular/asset_preprocess.py', 'missing_x_adjust=dropna', 'missing_y_adjust=none', 'outlier_adjust=none', 'data_scaler=standard', 'pickle_path=/']\u001b[0m\n",
      "\u001b[34m#033[94m=========================== METADATA STORE ============================#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mSTORE TYPE    : WORKFLOW#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mDATABASE TYPE : SQLITE#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mDATABASE FILE : /opt/program/framework/.storage/cad-train/cad-train.db#033[0m\u001b[0m\n",
      "\u001b[34m#033[94m=======================================================================#033[0m\u001b[0m\n",
      "\u001b[34m[METADATA][LOG][INFO] : version check success!!\u001b[0m\n",
      "\u001b[34m#033[94m========================== ASSET INFORMATION ==========================#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mTIME(KST)    : 2023-03-30 10:29:37#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKSPACE    : ai-advisor#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mPROJECT      : vs_telematcis#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKFLOW     : cad-train#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKFLOW KEY : vs_telematcis-cad-train-20230330012935#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mASSET NAME   : preprocess#033[0m\u001b[0m\n",
      "\u001b[34m#033[94masset ver.   : v.1.1.0#033[0m\u001b[0m\n",
      "\u001b[34m#033[94maiplib ver.  : 1.1.0#033[0m\u001b[0m\n",
      "\u001b[34m#033[94maiptfx ver.  : 0.6.4#033[0m\u001b[0m\n",
      "\u001b[34m#033[94m=======================================================================#033[0m\u001b[0m\n",
      "\u001b[34mLoaded : /opt/program/framework/workflows/vs_telematcis-cad-train-20230330012935/interface/validate.tfrecord\u001b[0m\n",
      "\u001b[34m************************************************************\u001b[0m\n",
      "\u001b[34m#011preprocess -> run\u001b[0m\n",
      "\u001b[34m************************************************************\u001b[0m\n",
      "\u001b[34m{'aiptfx': '0.6.4', 'aiplib': '1.1.0', 'asset': 'v.1.1.0'}\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mSaved : /opt/program/framework/workflows/vs_telematcis-cad-train-20230330012935/interface/preprocess.tfrecord\u001b[0m\n",
      "\u001b[34m#033[92m\u001b[0m\n",
      "\u001b[34m################################# STEP EXTRACT #####################################033[0m\n",
      " ARGUMENTS  : ['/opt/program/framework/scripts/feature_extract/adxai/asset_extract.py', 'pattern_column=cycle', 'max_pattern_length=', 'check_time_column=false']\u001b[0m\n",
      "\u001b[34m#033[94m=========================== METADATA STORE ============================#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mSTORE TYPE    : WORKFLOW#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mDATABASE TYPE : SQLITE#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mDATABASE FILE : /opt/program/framework/.storage/cad-train/cad-train.db#033[0m\u001b[0m\n",
      "\u001b[34m#033[94m=======================================================================#033[0m\u001b[0m\n",
      "\u001b[34m[METADATA][LOG][INFO] : version check success!!\u001b[0m\n",
      "\u001b[34m#033[94m========================== ASSET INFORMATION ==========================#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mTIME(KST)    : 2023-03-30 10:29:37#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKSPACE    : ai-advisor#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mPROJECT      : vs_telematcis#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKFLOW     : cad-train#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKFLOW KEY : vs_telematcis-cad-train-20230330012935#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mASSET NAME   : extract#033[0m\u001b[0m\n",
      "\u001b[34m#033[94masset ver.   : None#033[0m\u001b[0m\n",
      "\u001b[34m#033[94maiplib ver.  : 1.1.0#033[0m\u001b[0m\n",
      "\u001b[34m#033[94maiptfx ver.  : 0.6.4#033[0m\u001b[0m\n",
      "\u001b[34m#033[94m=======================================================================#033[0m\u001b[0m\n",
      "\u001b[34mLoaded : /opt/program/framework/workflows/vs_telematcis-cad-train-20230330012935/interface/preprocess.tfrecord\u001b[0m\n",
      "\u001b[34m************************************************************\u001b[0m\n",
      "\u001b[34m#011extract -> run\u001b[0m\n",
      "\u001b[34m************************************************************\u001b[0m\n",
      "\u001b[34m{'aiplib': '1.1.0', 'aiptfx': '0.6.4', 'asset': 'None'}\u001b[0m\n",
      "\u001b[34mvstack_columns 중 column(data) 이 입력 데이터에 존재함을 확인하였습니다. \u001b[0m\n",
      "\u001b[34mPattern column 중 column(cycle) 이 입력 데이터에 존재함을 확인하였습니다. \u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mColumn=data 을 pivot 하는데 소요된 시간은 0 초 입니다.\u001b[0m\n",
      "\u001b[34m데이터 의 type 을 float 으로 변경하는데 소요된 시간은 0 초 입니다.\u001b[0m\n",
      "\u001b[34mSaved : /opt/program/framework/workflows/vs_telematcis-cad-train-20230330012935/interface/extract.tfrecord\u001b[0m\n",
      "\u001b[34m#033[92m\u001b[0m\n",
      "\u001b[34m################################# STEP TRAIN #####################################033[0m\n",
      " ARGUMENTS  : ['/opt/program/framework/scripts/contextual_ad/asset_train.py', 'train_hpo=true', 'min_train_rows=5', 'max_train_rows=3000', 'model=rrcf2', 'apply_max_row=True', 'rrcf_target_size=50', 'rrcf_num_tree=100', 'rrcf_w=10', 'mp_target_window=50', 'mp_w=3']\u001b[0m\n",
      "\u001b[34m#033[94m=========================== METADATA STORE ============================#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mSTORE TYPE    : WORKFLOW#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mDATABASE TYPE : SQLITE#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mDATABASE FILE : /opt/program/framework/.storage/cad-train/cad-train.db#033[0m\u001b[0m\n",
      "\u001b[34m#033[94m=======================================================================#033[0m\u001b[0m\n",
      "\u001b[34m[METADATA][LOG][INFO] : version check success!!\u001b[0m\n",
      "\u001b[34m#033[94m========================== ASSET INFORMATION ==========================#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mTIME(KST)    : 2023-03-30 10:29:38#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKSPACE    : ai-advisor#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mPROJECT      : vs_telematcis#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKFLOW     : cad-train#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKFLOW KEY : vs_telematcis-cad-train-20230330012935#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mASSET NAME   : train#033[0m\u001b[0m\n",
      "\u001b[34m#033[94masset ver.   : None#033[0m\u001b[0m\n",
      "\u001b[34m#033[94maiplib ver.  : 1.1.0#033[0m\u001b[0m\n",
      "\u001b[34m#033[94maiptfx ver.  : 0.6.4#033[0m\u001b[0m\n",
      "\u001b[34m#033[94m=======================================================================#033[0m\u001b[0m\n",
      "\u001b[34mLoaded : /opt/program/framework/workflows/vs_telematcis-cad-train-20230330012935/interface/extract.tfrecord\u001b[0m\n",
      "\u001b[34m************************************************************\u001b[0m\n",
      "\u001b[34m#011train -> run\u001b[0m\n",
      "\u001b[34m************************************************************\u001b[0m\n",
      "\u001b[34m{'asset': 'None', 'aiplib': '1.1.0', 'aiptfx': '0.6.4'}\u001b[0m\n",
      "\u001b[34mRead config[x_columns] -> ['idx_030', 'idx_139', 'idx_014', 'idx_291', 'idx_088', 'idx_208', 'idx_152', 'idx_202', 'idx_248', 'idx_043', 'idx_098', 'idx_142', 'idx_031', 'idx_156', 'idx_194', 'idx_089', 'idx_232', 'idx_269', 'idx_193', 'idx_262', 'idx_091', 'idx_016', 'idx_048', 'idx_071', 'idx_187', 'idx_213', 'idx_158', 'idx_109', 'idx_241', 'idx_052', 'idx_234', 'idx_249', 'idx_060', 'idx_067', 'idx_281', 'idx_057', 'idx_102', 'idx_132', 'idx_206', 'idx_199', 'idx_058', 'idx_178', 'idx_074', 'idx_189', 'idx_243', 'idx_295', 'idx_101', 'idx_200', 'idx_046', 'idx_035', 'idx_068', 'idx_140', 'idx_069', 'idx_226', 'idx_242', 'idx_002', 'idx_186', 'idx_000', 'idx_051', 'idx_160', 'idx_003', 'idx_182', 'idx_128', 'idx_162', 'idx_258', 'idx_239', 'idx_116', 'idx_010', 'idx_008', 'idx_063', 'idx_185', 'idx_028', 'idx_251', 'idx_007', 'idx_225', 'idx_012', 'idx_072', 'idx_113', 'idx_275', 'idx_032', 'idx_179', 'idx_034', 'idx_147', 'idx_076', 'idx_125', 'idx_190', 'idx_154', 'idx_188', 'idx_137', 'idx_059', 'idx_120', 'idx_164', 'idx_207', 'idx_209', 'idx_223', 'idx_268', 'idx_282', 'idx_004', 'idx_100', 'idx_087', 'idx_121', 'idx_174', 'idx_222', 'idx_044', 'idx_215', 'idx_197', 'idx_254', 'idx_011', 'idx_150', 'idx_027', 'idx_177', 'idx_151', 'idx_083', 'idx_276', 'idx_259', 'idx_144', 'idx_227', 'idx_287', 'idx_192', 'idx_292', 'idx_256', 'idx_019', 'idx_138', 'idx_238', 'idx_122', 'idx_075', 'idx_273', 'idx_163', 'idx_107', 'idx_127', 'idx_263', 'idx_161', 'idx_271', 'idx_103', 'idx_064', 'idx_039', 'idx_224', 'idx_005', 'idx_021', 'idx_026', 'idx_104', 'idx_290', 'idx_153', 'idx_166', 'idx_294', 'idx_252', 'idx_105', 'idx_040', 'idx_096', 'idx_111', 'idx_093', 'idx_145', 'idx_165', 'idx_277', 'idx_033', 'idx_136', 'idx_228', 'idx_108', 'idx_261', 'idx_141', 'idx_081', 'idx_180', 'idx_168', 'idx_267', 'idx_130', 'idx_216', 'idx_253', 'idx_176', 'idx_148', 'idx_230', 'idx_218', 'idx_047', 'idx_085', 'idx_126', 'idx_247', 'idx_255', 'idx_001', 'idx_009', 'idx_022', 'idx_115', 'idx_117', 'idx_172', 'idx_181', 'idx_119', 'idx_041', 'idx_029', 'idx_061', 'idx_266', 'idx_045', 'idx_066', 'idx_184', 'idx_203', 'idx_233', 'idx_274', 'idx_175', 'idx_229', 'idx_094', 'idx_134', 'idx_124', 'idx_049', 'idx_080', 'idx_149', 'idx_171', 'idx_196', 'idx_169', 'idx_279', 'idx_296', 'idx_036', 'idx_212', 'idx_106', 'idx_079', 'idx_221', 'idx_146', 'idx_191', 'idx_265', 'idx_240', 'idx_297', 'idx_167', 'idx_013', 'idx_284', 'idx_129', 'idx_050', 'idx_056', 'idx_143', 'idx_278', 'idx_077', 'idx_195', 'idx_283', 'idx_070', 'idx_114', 'idx_272', 'idx_055', 'idx_260', 'idx_250', 'idx_201', 'idx_065', 'idx_110', 'idx_112', 'idx_037', 'idx_118', 'idx_038', 'idx_235', 'idx_090', 'idx_293', 'idx_078', 'idx_198', 'idx_286', 'idx_214', 'idx_017', 'idx_133', 'idx_159', 'idx_257', 'idx_097', 'idx_285', 'idx_006', 'idx_015', 'idx_131', 'idx_288', 'idx_082', 'idx_024', 'idx_220', 'idx_123', 'idx_298', 'idx_023', 'idx_246', 'idx_095', 'idx_086', 'idx_299', 'idx_245', 'idx_053', 'idx_092', 'idx_054', 'idx_155', 'idx_157', 'idx_217', 'idx_236', 'idx_237', 'idx_025', 'idx_099', 'idx_270', 'idx_264', 'idx_231', 'idx_183', 'idx_020', 'idx_280', 'idx_073', 'idx_204', 'idx_170', 'idx_018', 'idx_062', 'idx_135', 'idx_173', 'idx_211', 'idx_210', 'idx_205', 'idx_084', 'idx_219', 'idx_289', 'idx_042', 'idx_244'] \u001b[0m\n",
      "\u001b[34mRead config[time_column] -> time_fake_st \u001b[0m\n",
      "\u001b[34mRead config[group_keys] -> project \u001b[0m\n",
      "\u001b[34mGroup List: ['project']\u001b[0m\n",
      "\u001b[34m---------------- 1 / 1  학습을 시작 합니다. ----------------\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34m데이터 크기는 Row(s) 200 개, Column(s) 306 개 입니다. \u001b[0m\n",
      "\u001b[34mSaved : /opt/program/framework/workflows/vs_telematcis-cad-train-20230330012935/interface/train.tfrecord\u001b[0m\n",
      "\u001b[34m#033[92m\u001b[0m\n",
      "\u001b[34m################################# STEP OUTPUT #####################################033[0m\n",
      " ARGUMENTS  : ['/opt/program/framework/scripts/data_output/tabular/asset_output.py', 'output_path=/opt/program/framework/output/cad-train/', 'rename_file=none', 'rename_columns=none']\u001b[0m\n",
      "\u001b[34m#033[94m=========================== METADATA STORE ============================#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mSTORE TYPE    : WORKFLOW#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mDATABASE TYPE : SQLITE#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mDATABASE FILE : /opt/program/framework/.storage/cad-train/cad-train.db#033[0m\u001b[0m\n",
      "\u001b[34m#033[94m=======================================================================#033[0m\u001b[0m\n",
      "\u001b[34m[METADATA][LOG][INFO] : version check success!!\u001b[0m\n",
      "\u001b[34m#033[94m========================== ASSET INFORMATION ==========================#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mTIME(KST)    : 2023-03-30 10:32:05#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKSPACE    : ai-advisor#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mPROJECT      : vs_telematcis#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKFLOW     : cad-train#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mWORKFLOW KEY : vs_telematcis-cad-train-20230330012935#033[0m\u001b[0m\n",
      "\u001b[34m#033[94mASSET NAME   : output#033[0m\u001b[0m\n",
      "\u001b[34m#033[94masset ver.   : v.1.1.0#033[0m\u001b[0m\n",
      "\u001b[34m#033[94maiplib ver.  : 1.1.0#033[0m\u001b[0m\n",
      "\u001b[34m#033[94maiptfx ver.  : 0.6.4#033[0m\u001b[0m\n",
      "\u001b[34m#033[94m=======================================================================#033[0m\u001b[0m\n",
      "\u001b[34mLoaded : /opt/program/framework/workflows/vs_telematcis-cad-train-20230330012935/interface/train.tfrecord\u001b[0m\n",
      "\u001b[34m************************************************************\u001b[0m\n",
      "\u001b[34m#011output -> run\u001b[0m\n",
      "\u001b[34m************************************************************\u001b[0m\n",
      "\u001b[34m{'aiptfx': '0.6.4', 'aiplib': '1.1.0', 'asset': 'v.1.1.0'}\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34m[METADATA][LOG][INFO] : The results will be served /opt/program/framework/.output/cad-train/ to /opt/program/framework/output/cad-train/ folder.\u001b[0m\n",
      "\u001b[34mFinish Train Pipeline!\u001b[0m\n",
      "\u001b[34mTrain models moved into /opt/ml/model path!\u001b[0m\n",
      "\u001b[34mTrain output moved into model/train_output path!\u001b[0m\n",
      "\u001b[34mInference config yaml moved into model/inference_config path!\u001b[0m\n",
      "\n",
      "2023-03-30 01:32:26 Uploading - Uploading generated training model\n",
      "2023-03-30 01:33:12 Completed - Training job completed\n",
      "Training seconds: 357\n",
      "Billable seconds: 357\n"
     ]
    }
   ],
   "source": [
    "########################################CHANGE####################################################\n",
    "# Create an estimator object for running a training job\n",
    "estimator = sagemaker.algorithm.AlgorithmEstimator(\n",
    "    algorithm_arn=algo_arn,\n",
    "    base_job_name=\"aia-contextual-anomaly-detection\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge',\n",
    "    input_mode=\"File\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={},\n",
    ")\n",
    "##################################################################################################\n",
    "\n",
    "# Run the training job.\n",
    "estimator.fit({'training': training_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See this [blog-post](https://aws.amazon.com/blogs/machine-learning/easily-monitor-and-visualize-metrics-while-training-models-on-amazon-sagemaker/) for more information how to visualize metrics during the process. You can also open the training job from [Amazon SageMaker console](https://console.aws.amazon.com/sagemaker/home?#/jobs/) and monitor the metrics/logs in **Monitor** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy model and verify results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can deploy the model for performing real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################CHANGE####################################################\n",
    "model_name = \"aia_cad_inference_model\"\n",
    "content_type='text/csv'\n",
    "\n",
    "# set instance type\n",
    "real_time_inference_instance_type = 'ml.c5.xlarge'\n",
    "batch_transform_inference_instance_type = 'ml.c5.xlarge'\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Deploy trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "# deploy model\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type=real_time_inference_instance_type, serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################CHANGE####################################################\n",
    "result = pd.read_csv(\"inference_result.csv\", header=None)\n",
    "##################################################################################################\n",
    "\n",
    "# print result\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint is created, you can perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will perform batch inference using multiple input payloads together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Perform Batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################CHANGE####################################################\n",
    "# upload the batch-transform job input files to S3\n",
    "transform_input_folder = test_dataset\n",
    "##################################################################################################\n",
    "\n",
    "transform_input = sagemaker_session.upload_data(transform_input_folder, key_prefix=model_name)\n",
    "print(\"Transform input uploaded to : \" + transform_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the batch-transform job\n",
    "transformer = estimator.transformer(instance_count=1, instance_type=batch_transform_inference_instance_type)\n",
    "transformer.transform(transform_input, content_type=content_type)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output is available on following path\n",
    "transformer.output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################CHANGE####################################################\n",
    "result = pd.read_csv(\"inference_result.csv\", header=None)\n",
    "##################################################################################################\n",
    "\n",
    "# print result\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Add code snippet that shows the payload contents>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "\n",
    "runtime = boto3.client('sagemaker-runtime', region_name=aws_region, aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key)\n",
    "\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=predictor.endpoint_name,ContentType=content_type,Body=open(file_name, 'rb').read(),Accept=content_type\n",
    ")\n",
    "result = response['Body'].read().decode('utf-8').replace('\\x00', '')\n",
    "result_df = pd.read_csv(io.StringIO(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################CHANGE####################################################\n",
    "file_name = test_dataset\n",
    "output_file_name = \"inference_result.csv\"\n",
    "##################################################################################################\n",
    "\n",
    "!aws sagemaker-runtime invoke-endpoint \\\n",
    "    --endpoint-name $predictor.endpoint \\\n",
    "    --body fileb://$file_name \\\n",
    "    --content-type $content_type \\\n",
    "    --region $sagemaker_session.boto_region_name \\\n",
    "    --profile marketplace \\\n",
    "    $output_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.4 Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################CHANGE####################################################\n",
    "result = pd.read_csv(\"inference_result.csv\", header=None)\n",
    "##################################################################################################\n",
    "\n",
    "# print result\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. you can terminate the same to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Unsubscribe to the listing (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the algorithm, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
